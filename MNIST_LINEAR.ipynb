{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_LINEAR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq9fskZv-bOi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from tqdm import trange\n",
        "\n",
        "LOG_DIR = './logs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3VmN1TdDRhJ"
      },
      "source": [
        "def build_better_linear_model(model_name, learning_rate, batch_size, reuse=False, **kwargs): \n",
        "  def _linear(input_tensor, input_dim, output_dim, layer_name):\n",
        "    with tf.variable_scope(model_name + layer_name, reuse=tf.compat.v1.AUTO_REUSE):\n",
        "      with tf.variable_scope(\"weights\"):\n",
        "        W = tf.get_variable(\"W\", shape=(input_dim, output_dim))\n",
        "      with tf.variable_scope(\"biases\"):\n",
        "        b = tf.get_variable(\"b\", shape=(output_dim))\n",
        "      return tf.matmul(input_tensor, W) + b\n",
        "    \n",
        "  def _linear_with_normal(input_tensor, input_dim, output_dim, layer_name):  \n",
        "    with tf.variable_scope(model_name + layer_name, reuse=tf.compat.v1.AUTO_REUSE):\n",
        "      with tf.variable_scope(\"weights\"):\n",
        "        W = tf.get_variable(\"W\", shape=(input_dim, output_dim))\n",
        "      with tf.variable_scope(\"gamma\"):\n",
        "        gam = tf.get_variable(\"gamma\", shape=(output_dim))\n",
        "      with tf.variable_scope(\"beta\"):\n",
        "        beta = tf.get_variable(\"beta\", shape=(output_dim))\n",
        "      return tf.matmul(input_tensor, W) * (gam+1) + beta\n",
        "       \n",
        "  class Model(object):\n",
        "    pass\n",
        "  \n",
        "  model = Model()\n",
        "  img_size = kwargs[\"image_size\"]\n",
        "  \n",
        "  images = 1\n",
        "  \n",
        "  if len(img_size) == 2:\n",
        "    images = tf.placeholder(shape=(None, None, None), dtype=tf.float32, name=\"images\")\n",
        "  else:\n",
        "    images = tf.placeholder(shape=(None, None, None, None), dtype=tf.float32, name=\"images\")\n",
        "\n",
        "  labels = tf.placeholder(shape=(None), dtype=tf.int32, name=\"labels\")\n",
        "  \n",
        "  model.inputs = [images, labels]\n",
        "  \n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "  train_dataset = train_dataset.shuffle(1000)\n",
        "  train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
        "  \n",
        "  train_dataset = train_dataset.repeat()\n",
        "  \n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "  test_dataset = test_dataset.batch(batch_size, drop_remainder = True)\n",
        "  \n",
        "  model.train_iterator = train_dataset.make_initializable_iterator()\n",
        "  model.test_iterator = test_dataset.make_initializable_iterator()\n",
        "  \n",
        "  model.handle = tf.placeholder(tf.string, shape=[])\n",
        "  model.iterator = tf.data.Iterator.from_string_handle(\n",
        "      model.handle, train_dataset.output_types, train_dataset.output_shapes)\n",
        "  \n",
        "  images, labels = model.iterator.get_next()\n",
        "  \n",
        "\n",
        "  # Flatten image\n",
        "  x = tf.reshape(images, [-1,  kwargs[\"pixels\"]])\n",
        "  sizes = kwargs[\"sizes\"]\n",
        "  last_size = kwargs[\"pixels\"]\n",
        "  for id, size in enumerate(sizes):\n",
        "    x = _linear_with_normal(input_tensor=x, \n",
        "                     input_dim=last_size, \n",
        "                     output_dim=size , \n",
        "                     layer_name='linear1'+str(id))\n",
        "\n",
        "    x = tf.nn.relu(x)\n",
        "    if reuse==False:\n",
        "      x = tf.nn.dropout(x, keep_prob=(1-kwargs[\"dropout\"]))\n",
        "    last_size = size\n",
        "  \n",
        "  logits = _linear(input_tensor = x, \n",
        "                  input_dim = last_size,\n",
        "                  output_dim = 10,\n",
        "                  layer_name = 'linear21')\n",
        "\n",
        "  model.probs = tf.nn.softmax(logits)\n",
        "  \n",
        "  # Compute loss\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    model.loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, \n",
        "                                                        logits=logits)\n",
        "  \n",
        "  # Compute accuracy\n",
        "  with tf.variable_scope(\"accuracy\"):\n",
        "    model.correct_predictions = tf.equal(\n",
        "        labels, tf.argmax(model.probs, axis=1, output_type=tf.int32))\n",
        "    accuracy = tf.reduce_mean(tf.cast(model.correct_predictions, tf.float32))\n",
        "\n",
        "  # Optimize\n",
        "  model.train_op = tf.train.GradientDescentOptimizer(\n",
        "      learning_rate=learning_rate).minimize(model.loss)\n",
        " \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU4MUo5I0w-e"
      },
      "source": [
        "def evaluate_better_linear_model(sess, model, test_images, test_labels):\n",
        "  images, labels = model.inputs\n",
        "  sess.run(model.test_iterator.initializer, feed_dict={\n",
        "      images: test_images,\n",
        "      labels: test_labels\n",
        "  })\n",
        "  test_handle = sess.run(model.test_iterator.string_handle())\n",
        "  \n",
        "  correct_predictions = 0.0\n",
        "  total_predictions = 0\n",
        "  while True:\n",
        "    try:\n",
        "      predictions = sess.run(model.correct_predictions,\n",
        "                             feed_dict={model.handle: test_handle})\n",
        "      correct_predictions += np.sum(predictions)\n",
        "      total_predictions += predictions.shape[0]\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      break\n",
        "\n",
        "  return correct_predictions / total_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqVnZq7XPJb1"
      },
      "source": [
        "def _create_nn(model_name, train_images, train_labels, test_images, test_labels, **params):\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  model = build_better_linear_model(model_name = model_name, **params)\n",
        "  eval_model = build_better_linear_model(model_name = model_name, reuse=True, **params)\n",
        "\n",
        "  timestamp = int(time.time())\n",
        "  writer = tf.summary.FileWriter(\n",
        "      os.path.join(LOG_DIR, 'better_linear_model_%d' % timestamp), \n",
        "      graph = tf.get_default_graph())\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    train_handle = sess.run(model.train_iterator.string_handle()) #zwroc iterator do danych treningowych\n",
        "    \n",
        "    images, labels = model.inputs\n",
        "    sess.run(model.train_iterator.initializer, feed_dict={\n",
        "        images: train_images,\n",
        "        labels: train_labels\n",
        "    })\n",
        "\n",
        "    steps_per_epoch = train_images.shape[0] // params['batch_size'] \n",
        "\n",
        "    for epoch in range(params['num_epochs']):\n",
        "      for i in range(steps_per_epoch):\n",
        "        step = epoch * steps_per_epoch + i\n",
        "        sess.run(model.train_op,\n",
        "                   feed_dict={model.handle: train_handle})\n",
        "\n",
        "      print(\"Epoch %d: %.4f\" % (\n",
        "          epoch, evaluate_better_linear_model(sess, eval_model, test_images, test_labels)))\n",
        "\n",
        "  writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IOtX_1y4RhK"
      },
      "source": [
        "params = {\n",
        "    'batch_size': 128, \n",
        "    'num_epochs': 50,\n",
        "    'learning_rate': 0.1,\n",
        "    'log_summaries_every': 100,\n",
        "    'dropout': 0.1,\n",
        "    'sizes': [500],\n",
        "    'image_size': (28, 28),\n",
        "    'pixels':784\n",
        "}\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = np.array(train_images, dtype=np.float32) / 255\n",
        "test_images = np.array(test_images, dtype=np.float32) / 255\n",
        "\n",
        "train_images.reshape(train_images.shape[0], 784)\n",
        "\n",
        "train_labels = np.array(train_labels, dtype=np.int32)\n",
        "test_labels = np.array(test_labels, dtype=np.int32)\n",
        "test_images.reshape(test_images.shape[0], 784)\n",
        "\n",
        "_create_nn(\"mnist\", train_images, train_labels, test_images, test_labels, \n",
        "           **params)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}