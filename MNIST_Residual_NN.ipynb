{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST_Residual_NN.ipynb","provenance":[{"file_id":"1tu4_7avB51Ck0tKtf5XfM2_DBoHiAmbD","timestamp":1542093030243}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZUhHGoTIqphr"},"source":["!pip install progressbar2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import time\n","import progressbar\n","from keras.datasets import mnist, cifar10\n","\n","from tqdm import trange"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ysGeguvp-Fo","executionInfo":{"status":"ok","timestamp":1604934547091,"user_tz":-60,"elapsed":1667,"user":{"displayName":"Paweł Mader","photoUrl":"","userId":"11608580188655098713"}}},"source":["def build_linear_model_two(is_training, learning_rate, reuse, **kwargs):\n","  with tf.variable_scope('', reuse=reuse):  \n","    def _linear_with_bias(input_tensor, input_dim, output_dim, layer_name):\n","      with tf.variable_scope(layer_name):\n","        with tf.variable_scope(\"weights\"):\n","          W = tf.get_variable(\"W\", shape=(input_dim, output_dim))\n","\n","        with tf.variable_scope(\"biases\"):\n","          b = tf.get_variable(\"b\", shape=(output_dim))\n","\n","        return tf.matmul(input_tensor, W) + b\n","\n","    def _linear_with_normalization(input_tensor, input_dim, output_dim, layer_name, eps=1e-9):\n","      with tf.variable_scope(layer_name):\n","        with tf.variable_scope(\"weights\"):\n","          W = tf.get_variable(\"W\", shape=(input_dim, output_dim))\n","\n","        with tf.variable_scope(\"gamma\"):\n","          gamma = tf.get_variable(\"gamma\", shape=(1, output_dim))\n","\n","        with tf.variable_scope(\"beta\"):   \n","          beta = tf.get_variable(\"beta\", shape=(1, output_dim))\n","        x = tf.matmul(input_tensor, W)\n","        mean, var = tf.nn.moments(x, axes=[1], keep_dims=True)\n","        return (x-mean)/tf.math.sqrt(var + eps) * (gamma+1) + beta\n","        \n","    def _normalization(input_tensor, input_dim, layer_name, number, eps = 1e-9):\n","      with tf.variable_scope(layer_name+\".\"+str(number)):\n","        with tf.variable_scope(\"gamma\"):\n","          gamma = tf.get_variable(\"gamma\", shape=(1, 1, 1, input_dim[3]))\n","\n","        with tf.variable_scope(\"beta\"):   \n","          beta = tf.get_variable(\"beta\", shape=(1, 1, 1, input_dim[3]))\n","\n","        mean, var = tf.nn.moments(input_tensor, axes=[0,1,2], keep_dims=True)\n","        return (input_tensor-mean)/tf.math.sqrt(var + eps) * (gamma+1) + beta\n","        \n","    def _convolution(input_tensor, output_dim, layer_name, number):\n","      with tf.variable_scope(layer_name+str(number)):\n","        with tf.variable_scope(\"conv\"):\n","          filters = tf.get_variable(\"filters\", shape=(output_dim[0], output_dim[1], output_dim[2], output_dim[3]))\n","\n","      return tf.nn.conv2d(input_tensor, filters, [1, 1, 1, 1], padding=\"SAME\")\n","      \n","    def _activation(input_tensor):\n","      return tf.nn.relu(input_tensor)\n","    \n","    def _dropout(input_tensor, dropout, training):\n","      if training:\n","        return tf.nn.dropout(input_tensor, 1.0-dropout)\n","      else:\n","        return input_tensor\n","    \n","    class Model(object):\n","      pass\n","\n","    model = Model()\n","\n","    # Define input\n","    image_size = kwargs[\"image_size\"]\n","    images = tf.placeholder(shape=(None, image_size[0], image_size[1], image_size[2]), dtype=tf.float32, name=\"images\")\n","    labels = tf.placeholder(shape=(None), dtype=tf.int32, name=\"labels\")\n","\n","    model.inputs = [images, labels]\n","\n","    tf.summary.image('input_images', images)\n","\n","    x = images\n","    mean_b, var_b = tf.nn.moments(x, axes=[1, 2], keep_dims=True)\n","    x = (x-mean_b)/tf.math.sqrt(var_b + 1e-9)\n","    sizes = kwargs[\"sizes\"]\n","    last_size = image_size[2]\n","    tmp_size = image_size[1]\n","    for i in range(len(sizes)):\n","      rx = x\n","      x = _convolution(x, (3, 3, last_size, sizes[i][0]), \"layer\"+str(i), 1)\n","      x = _normalization(x, x.shape, \"layer\"+str(i), 1)\n","      x = _activation(x)\n","      x = _dropout(x, kwargs[\"dropout\"], is_training)      \n","      \n","      if(last_size < sizes[i][1]):\n","        rx = _convolution(rx, (1, 1, last_size, sizes[i][1]), \"layer\" + str(i), 4)\n","        last_size = sizes[i][1]\n","      \n","      x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n","      rx = tf.nn.max_pool(rx, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n","      tmp_size = tmp_size/2\n","      \n","      x = rx + x\n","\n","\n","    x = tf.reshape(x, (kwargs[\"batch_size\"], -1))   \n","    x = _linear_with_normalization(x, input_dim=tmp_size*tmp_size*last_size, output_dim=500, layer_name='linear'+str(len(sizes))) \n","    x = _linear_with_normalization(x, input_dim=500, output_dim=100, layer_name='linear'+str(len(sizes)+1)) \n","    logits = _linear_with_bias(input_tensor=x, input_dim=100, output_dim=10, layer_name='linear'+str(len(sizes)+2))\n","                               \n","    # Compute probabilities\n","    model.probs = tf.nn.softmax(logits)\n","\n","    # Compute loss\n","    model.loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, \n","                                                        logits=logits)\n","\n","    # Optimize\n","    model.train_op = tf.train.AdamOptimizer().minimize(model.loss)\n","    return model"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9rJ-1x5qCYW","executionInfo":{"status":"ok","timestamp":1604934549455,"user_tz":-60,"elapsed":679,"user":{"displayName":"Paweł Mader","photoUrl":"","userId":"11608580188655098713"}}},"source":["def evaluate_linear_model(sess, model, test_images, test_labels, batch_size, **kwargs):\n","  steps_per_epoch = test_images.shape[0] // batch_size\n","  \n","  correct_predictions = 0.0\n","  total_predictions = 0\n","  for i in range(steps_per_epoch):\n","    batch_x = test_images[i*batch_size : (i+1)*batch_size]\n","    batch_y = test_labels[i*batch_size : (i+1)*batch_size]\n","    \n","    images, labels = model.inputs\n","    model_probs = sess.run(model.probs, feed_dict={\n","                                images: batch_x,\n","                                labels: batch_y,\n","                           })\n","\n","    test_labels = np.array(test_labels, dtype=np.int32).reshape(test_labels.shape[0])\n","    correct_predictions += np.sum(np.argmax(model_probs, axis=1) == batch_y)\n","    total_predictions += batch_size\n","    \n","  return correct_predictions / total_predictions"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7vNqmgKqLmf","executionInfo":{"status":"ok","timestamp":1604934551123,"user_tz":-60,"elapsed":706,"user":{"displayName":"Paweł Mader","photoUrl":"","userId":"11608580188655098713"}}},"source":["def _create_nn(train_images, train_labels, test_images, test_labels, **params):\n","  tf.reset_default_graph()\n","\n","  train_model = build_linear_model_two(is_training=True, reuse=False, **params)\n","  eval_model = build_linear_model_two(is_training=False, reuse=True, **params)\n","\n","  \n","  with tf.Session() as sess:\n","    \n","    sess.run(tf.global_variables_initializer())\n","    timestamp = int(time.time())\n","    writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'linear_model_%d' % timestamp), \n","                                   sess.graph,\n","                                   flush_secs=5)\n","  \n","\n","    steps_per_epoch = train_images.shape[0] // params['batch_size'] \n","\n","    loss_history = []\n","    for epoch in range(params['num_epochs']):    \n","      perm = np.random.permutation(train_images.shape[0])\n","      train_images = train_images[perm]\n","      train_labels = train_labels[perm]\n","      with progressbar.ProgressBar(max_value = steps_per_epoch) as bar:\n","        for i in range(steps_per_epoch):\n","          bar.update(i)\n","          step = epoch * steps_per_epoch + i\n","\n","          batch_x = train_images[i*params['batch_size'] : (i+1)*params['batch_size']]\n","          batch_y = train_labels[i*params['batch_size'] : (i+1)*params['batch_size']]\n","          images, labels = train_model.inputs\n","          \n","          _ = sess.run([train_model.train_op], feed_dict={\n","                images: batch_x,\n","                labels: batch_y,\n","          })\n","\n","        print(\"Epoch %d: %.4f\" % (\n","            epoch, evaluate_linear_model(sess, eval_model, test_images, test_labels,  **params)))\n","    \n","  writer.close()"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9cEJZCPqN1B"},"source":["params = {\n","    'batch_size': 64, \n","    'num_epochs': 30,\n","    'learning_rate': 0.01,\n","    'log_summaries_every': 200,\n","    'dropout': 0.1,\n","    'sizes': [(28, 28, 1), (64, 64, 1)],\n","    'image_size': (28, 28, 1),\n","    'pixels': 3072\n","\n","}\n","mnist = tf.keras.datasets.mnist\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","train_images = np.array(train_images, dtype=np.float32) / 255\n","test_images = np.array(test_images, dtype=np.float32) / 255\n","\n","train_images = train_images.reshape(-1, 28, 28, 1)\n","test_images = test_images.reshape(-1, 28, 28, 1)\n","print(train_images.shape, test_images.shape)\n","\n","\n","train_labels = np.array(train_labels, dtype=np.int32).reshape(train_labels.shape[0])\n","test_labels = np.array(test_labels, dtype=np.int32).reshape(test_labels.shape[0])\n","\n","_create_nn(train_images, train_labels, test_images, test_labels, **params)"],"execution_count":null,"outputs":[]}]}